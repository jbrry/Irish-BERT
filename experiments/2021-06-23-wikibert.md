
# Analysis of Continued Pre-training of WikiBERT

In the experiments below, the initial WikiBERT-ga checkpoint is continuously pretrained on our best corpus setting.


<img src="/assets/images/ga_BERT_wikibert_dependencies_LAS.png" style="display: block; margin: 0 auto" />

<img src="/assets/images/ga_BERT_wikibert_feats_accuracy.png" style="display: block; margin: 0 auto" />

<img src="/assets/images/ga_BERT_wikibert_upos_accuracy.png" style="display: block; margin: 0 auto" />

<img src="/assets/images/ga_BERT_wikibert_xpos_accuracy.png" style="display: block; margin: 0 auto" />
