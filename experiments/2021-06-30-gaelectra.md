
# Analysis of gaELECTRA

In the experiments below, the ELECTRA Base model is trained on our best corpus setting.
NOTE: training was interrupted at step 923k/1m and the training was resumed. The 1m checkpoint has a very low-scoring run with the parser (to be investigated further)


<img src="/assets/images/ga_BERT_gaelectra_dependencies_LAS.png" style="display: block; margin: 0 auto" />

<img src="/assets/images/ga_BERT_gaelectra_feats_accuracy.png" style="display: block; margin: 0 auto" />

<img src="/assets/images/ga_BERT_gaelectra_upos_accuracy.png" style="display: block; margin: 0 auto" />

<img src="/assets/images/ga_BERT_gaelectra_xpos_accuracy.png" style="display: block; margin: 0 auto" />
